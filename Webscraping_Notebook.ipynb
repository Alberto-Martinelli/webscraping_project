{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Webscraping and Applied ML - Project\n",
    "_Authors_: Alessia SARRITZU, Alberto MARTINELLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 1: Get hotel data from Foursquare\n",
    "* Import the necessary libraries\n",
    "* Define the necessary functions for the API calls and data retrival\n",
    "* Retrieve and save the data in .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_api_key():\n",
    "    # Load environment variables from the .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    # Get the API key from the environment variable\n",
    "    api_key = os.getenv('API_KEY')  # Fetch the API key\n",
    "\n",
    "    if api_key is None:\n",
    "        print(\"API key is missing in the .env file!\")\n",
    "        exit()\n",
    "    return api_key\n",
    "\n",
    "def convert_json_to_df(data):\n",
    "    if data:\n",
    "        flattened_data = []\n",
    "\n",
    "        # Check if 'results' key exists (Type 1 JSON structure)\n",
    "        if isinstance(data, dict) and 'results' in data:\n",
    "            places = data['results']\n",
    "        # Otherwise, assume it's a list (Type 2 JSON structure)\n",
    "        else:\n",
    "            places = data\n",
    "\n",
    "        # Iterate over each place in 'places' (works for both Type 1 and Type 2)\n",
    "        for place in places:\n",
    "            # Extract the required fields based on Type 1 or Type 2 structure\n",
    "            if 'fsq_id' in place:  # Type 1 (place has 'fsq_id' and 'categories')\n",
    "                place_data = {\n",
    "                    'fsq_id': place['fsq_id'],\n",
    "                    'name': place['name'],\n",
    "                    'address': place['location']['address'] if 'location' in place else None,\n",
    "                    'locality': place['location']['locality'] if 'location' in place else None,\n",
    "                    'country': place['location']['country'] if 'location' in place else None,\n",
    "                    'formatted_address': place['location']['formatted_address'] if 'location' in place else None,\n",
    "                    'latitude': place['geocodes']['main']['latitude'] if 'geocodes' in place else None,\n",
    "                    'longitude': place['geocodes']['main']['longitude'] if 'geocodes' in place else None,\n",
    "                    'distance': place['distance'] if 'distance' in place else None,\n",
    "                    'link': place['link'],\n",
    "                    'categories': [category['name'] for category in place['categories']]  # Extract category names\n",
    "                }\n",
    "            elif 'id' in place:  # Type 2 (place has 'id' and 'text')\n",
    "                place_data = {\n",
    "                    'id': place['id'],\n",
    "                    'created_at': place['created_at'],\n",
    "                    'text': place['text']\n",
    "                }\n",
    "            flattened_data.append(place_data)\n",
    "\n",
    "        # Convert the list of flattened data into a DataFrame\n",
    "        df = pd.DataFrame(flattened_data)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"No data available to save.\")\n",
    "\n",
    "def convert_df_to_csv(df, filename):\n",
    "    # Save the DataFrame to CSV\n",
    "    df.to_csv(filename, index=False)\n",
    "    # print(\"CSV file has been saved as 'output.csv'\")\n",
    "\n",
    "def get_json_data_from_api(url, api_calls):\n",
    "    api_key = get_api_key()\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Authorization\": api_key\n",
    "    }\n",
    "\n",
    "    # Make the GET request to the API\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()  # Parse the JSON response\n",
    "        # print(data)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "    api_calls[0] += 1\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "api_calls = [0]\n",
    "\n",
    "# Get the HOTELS in newyork near 124 Hudson Street in a radiues of 150 meters\n",
    "url = \"https://api.foursquare.com/v3/places/search?query=hotel&ll=40.720276093678535%2C-74.00855578601094&limit=50\"\n",
    "data = get_json_data_from_api(url, api_calls)\n",
    "\n",
    "df = convert_json_to_df(data)\n",
    "\n",
    "#for each fsq_id value in the fsq_id column, make an API call to get the tips for that place\n",
    "# and then add the tips to the dataframe\n",
    "for fsq_id in df['fsq_id']:\n",
    "    url = f\"https://api.foursquare.com/v3/places/{fsq_id}/tips\"\n",
    "    tips = get_json_data_from_api(url, api_calls)\n",
    "\n",
    "    tips_text = [tip['text'] for tip in tips]  # Get the 'text' from each tip\n",
    "\n",
    "    # Assign the list of tip texts to the dataframe\n",
    "    df.loc[df['fsq_id'] == fsq_id, 'tips'] = ', '.join(tips_text)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "convert_df_to_csv(df, 'hotel_reviews_API.csv')\n",
    "print(\"CSV file has been saved as 'output.csv'\")\n",
    "print(\"API calls made:\", api_calls[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 2: Get hotel data from KAYAK - Web Scraping\n",
    "* Import the necessary libraries\n",
    "* Define the necessary function for the Playwright and data retrival\n",
    "* Retrieve and save the data in .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_reviews_with_playwright_async(hotel_url):\n",
    "    reviews = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)  # Use headless=True for performance\n",
    "        page = await browser.new_page()\n",
    "        await page.goto(hotel_url)\n",
    "\n",
    "        try:\n",
    "            if await page.locator(\"button:has-text('Accept')\").is_visible():\n",
    "                await page.locator(\"button:has-text('Accept')\").click()\n",
    "\n",
    "            hotel_name_element = await page.query_selector(\"h1.c3xth-hotel-name\")\n",
    "            hotel_name = (await hotel_name_element.inner_text()).strip() if hotel_name_element else \"Unknown Hotel\"\n",
    "\n",
    "            await page.wait_for_selector(\".acD_-reviews-row-header\")\n",
    "\n",
    "            review_elements = await page.query_selector_all(\".acD_\")\n",
    "            for review in review_elements:\n",
    "                try:\n",
    "                    rating_element = await review.query_selector(\".wdjx-positive\")\n",
    "                    rating = (await rating_element.inner_text()).strip() if rating_element else None\n",
    "\n",
    "                    score_description_element = await review.query_selector(\".acD_-score-description\")\n",
    "                    score_description = (\n",
    "                        await score_description_element.inner_text()).strip() if score_description_element else None\n",
    "\n",
    "                    user_name_date_element = await review.query_selector(\".acD_-userName\")\n",
    "                    user_name_date = (\n",
    "                        await user_name_date_element.inner_text()).strip() if user_name_date_element else None\n",
    "\n",
    "                    pros_element = await review.query_selector(\".acD_-pros\")\n",
    "                    pros = (await pros_element.inner_text()).strip() if pros_element else None\n",
    "\n",
    "                    full_review_element = await review.query_selector(\"span[id^='showMoreText']\")\n",
    "                    full_review = (await full_review_element.inner_text()).strip() if full_review_element else None\n",
    "\n",
    "                    reviews.append({\n",
    "                        \"Hotel Name\": hotel_name,\n",
    "                        \"Rating\": rating,\n",
    "                        \"Score Description\": score_description,\n",
    "                        \"User and Date\": user_name_date,\n",
    "                        \"Pros\": pros,\n",
    "                        \"Full Review\": full_review,\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing review: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {hotel_url}: {e}\")\n",
    "        finally:\n",
    "            await browser.close()\n",
    "\n",
    "    return reviews\n",
    "\n",
    "async def scrape():\n",
    "    links_file = \"kayak_hotel_links.txt\"  # Ensure this file exists\n",
    "    all_reviews = []\n",
    "\n",
    "    with open(links_file, 'r') as file:\n",
    "        hotel_urls = file.read().splitlines()\n",
    "\n",
    "        for hotel_url in hotel_urls:\n",
    "            reviews_data = await scrape_reviews_with_playwright_async(hotel_url)\n",
    "\n",
    "            if reviews_data:\n",
    "                all_reviews.extend(reviews_data)\n",
    "\n",
    "    if all_reviews:\n",
    "        reviews_df = pd.DataFrame(all_reviews)\n",
    "        reviews_df.to_csv(\"hotel_reviews_playwright_l.csv\", index=False)\n",
    "\n",
    "# Run the scraper in Jupyter Notebook\n",
    "await scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step 3: Visualize a data preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "def display_data_preview(filename):\n",
    "    pd.set_option('display.max_columns', 200)\n",
    "    pd.set_option('display.max_rows', 200)\n",
    "    pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "    df = pd.read_csv(filename)\n",
    "    print(\"Dataframe preview:\")\n",
    "    print(tabulate(df, headers='keys', tablefmt='psql'))\n",
    "\n",
    "display_data_preview('hotel_reviews_API.csv')\n",
    "display_data_preview('hotel_reviews_playwright.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 4: Aggregate API and Scraping reviews for the same hotels in a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "playwright_file = \"hotel_reviews_playwright.csv\"\n",
    "playwright_df = pd.read_csv(playwright_file)\n",
    "\n",
    "# Group and aggregate the 'Full Review' column by 'Hotel Name'\n",
    "aggregated_reviews = (\n",
    "    playwright_df.groupby('Hotel Name', sort=False)['Full Review']\n",
    "        .apply(lambda x: ' | '.join(x))\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "aggregated_reviews.rename(columns={'Full Review': 'Aggregated Reviews'}, inplace=True)\n",
    "aggregated_reviews.to_csv(\"aggregated_playwright_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "playwright_file = \"aggregated_playwright_reviews.csv\"\n",
    "api_file = \"hotel_reviews_API.csv\"\n",
    "\n",
    "playwright_df = pd.read_csv(playwright_file)\n",
    "api_df = pd.read_csv(api_file)\n",
    "\n",
    "# Remove specified rows from the API dataset (The data corresponding to the following was not found on Kayak)\n",
    "excluded_hotels = [\n",
    "    \"Lobby Bar at Ace Hotel New York\",\n",
    "    \"The Bar Room at Temple Court\",\n",
    "    \"Mr. Purple\",\n",
    "    \"The Lobby\",\n",
    "    \"Bemelmans Bar\"\n",
    "]\n",
    "api_df = api_df[~api_df['name'].isin(excluded_hotels)].reset_index(drop=True)\n",
    "\n",
    "api_df['kayak'] = playwright_df['Aggregated Reviews']\n",
    "api_df.to_csv(\"api_playwright_aggregated_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
